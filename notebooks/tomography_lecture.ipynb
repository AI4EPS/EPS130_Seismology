{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce43184c",
   "metadata": {},
   "source": [
    "# Seismic Tomography\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/AI4EPS/EPS130_Seismology/blob/main/notebooks/tomography_lecture.ipynb\">\n",
    "<img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "\n",
    "In the travel times lecture, we solved the **forward problem**: given a velocity model, compute travel times. Now we flip the question — given observed travel times, can we **recover the velocity structure**? This is the **inverse problem**, and it's the foundation of seismic tomography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68367df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install netCDF4 cartopy\n",
    "\n",
    "# Download data files\n",
    "import os, tarfile, urllib.request, json\n",
    "data_url = \"https://github.com/AI4EPS/EPS130_Seismology/releases/download/tomography-data/tomography_data.tar.gz\"\n",
    "if not os.path.exists(\"tomography_data\"):\n",
    "    print(\"Downloading tomography data...\")\n",
    "    urllib.request.urlretrieve(data_url, \"tomography_data.tar.gz\")\n",
    "    with tarfile.open(\"tomography_data.tar.gz\") as tf:\n",
    "        tf.extractall()\n",
    "    os.remove(\"tomography_data.tar.gz\")\n",
    "    print(\"Done.\")\n",
    "\n",
    "# Download plate boundary data (Bird, 2003)\n",
    "pb_file = \"tomography_data/PB2002_boundaries.json\"\n",
    "if not os.path.exists(pb_file):\n",
    "    print(\"Downloading plate boundaries...\")\n",
    "    pb_url = \"https://raw.githubusercontent.com/fraxen/tectonicplates/master/GeoJSON/PB2002_boundaries.json\"\n",
    "    urllib.request.urlretrieve(pb_url, pb_file)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9c0df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mwl7twqfba",
   "metadata": {},
   "source": [
    "The cell below defines helper functions used throughout this notebook. **You don't need to read this code** — just run it. Here's what each function does:\n",
    "\n",
    "- `build_G(...)` — Computes the path-length matrix $\\mathbf{G}$ for 1D layered models (used in Section 1)\n",
    "- `smoothing_matrix_1d(n)` — Builds the smoothing matrix $\\mathbf{L}$ for 1D (differences between adjacent layers)\n",
    "- `smoothing_matrix_2d(nx, nz)` — Builds the smoothing matrix $\\mathbf{L}$ for 2D grids (differences between adjacent blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b69e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_G(p_values, v_true, dz):\n",
    "    \"\"\"Build the path-length matrix G for 1D layers.\n",
    "    G[i,j] = path length of ray i in layer j.\"\"\"\n",
    "    n_rays, n_layers = len(p_values), len(v_true)\n",
    "    G = np.zeros((n_rays, n_layers))\n",
    "    for i, p in enumerate(p_values):\n",
    "        for j in range(n_layers):\n",
    "            if p >= 1.0 / v_true[j]:\n",
    "                break\n",
    "            cos_theta = np.sqrt(1 - (p * v_true[j])**2)\n",
    "            G[i, j] = 2 * dz[j] / cos_theta\n",
    "    return G\n",
    "\n",
    "def smoothing_matrix_1d(n):\n",
    "    \"\"\"Build 1D smoothing matrix L: differences between adjacent layers.\n",
    "    For 4 layers: L is 3x4, each row computes (layer[j+1] - layer[j]).\"\"\"\n",
    "    L = np.zeros((n - 1, n))\n",
    "    for i in range(n - 1):\n",
    "        L[i, i] = -1\n",
    "        L[i, i + 1] = 1\n",
    "    return L\n",
    "\n",
    "def smoothing_matrix_2d(nx, nz):\n",
    "    \"\"\"Build 2D smoothing matrix L: differences between adjacent blocks.\n",
    "    Penalizes differences between horizontal and vertical neighbors.\"\"\"\n",
    "    n = nx * nz\n",
    "    rows = []\n",
    "    for iz in range(nz):\n",
    "        for ix in range(nx):\n",
    "            k = iz * nx + ix\n",
    "            if ix < nx - 1:  # horizontal neighbor\n",
    "                row = np.zeros(n)\n",
    "                row[k] = -1\n",
    "                row[k + 1] = 1\n",
    "                rows.append(row)\n",
    "            if iz < nz - 1:  # vertical neighbor\n",
    "                row = np.zeros(n)\n",
    "                row[k] = -1\n",
    "                row[k + nx] = 1\n",
    "                rows.append(row)\n",
    "    return np.array(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ca961c",
   "metadata": {},
   "source": [
    "## 1. The Inverse Problem: 1D Inversion\n",
    "\n",
    "Consider rays traveling through horizontal layers. The travel time of ray $i$ is:\n",
    "\n",
    "$$t_i = \\sum_{j=1}^{M} G_{ij} \\, s_j$$\n",
    "\n",
    "where $s_j = 1/v_j$ is the **slowness** (what we want to find) and $G_{ij}$ is the **path length** of ray $i$ in layer $j$. In matrix form: $\\mathbf{Gm} = \\mathbf{d}$.\n",
    "\n",
    "| Symbol | Meaning | Size |\n",
    "|--------|---------|------|\n",
    "| $\\mathbf{G}$ | Path-length matrix | $N_{\\text{rays}} \\times M_{\\text{layers}}$ |\n",
    "| $\\mathbf{m}$ | Slowness in each layer | $M_{\\text{layers}} \\times 1$ |\n",
    "| $\\mathbf{d}$ | Observed travel times | $N_{\\text{rays}} \\times 1$ |\n",
    "\n",
    "We set up a 4-layer model with 6 rays at different ray parameters $p$. The path length through each layer is:\n",
    "\n",
    "$$\\ell_{ij} = \\frac{2 \\, \\Delta z_j}{\\sqrt{1 - p_i^2 v_j^2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1894b96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True model (unknown to the \"observer\")\n",
    "v_true = np.array([4.0, 5.5, 7.0, 8.0])  # km/s\n",
    "s_true = 1.0 / v_true                      # slowness (s/km)\n",
    "dz = np.array([3.0, 4.0, 5.0, 6.0])       # layer thicknesses (km)\n",
    "n_layers = len(v_true)\n",
    "\n",
    "# Ray parameters for 6 rays (s/km)\n",
    "p_values = np.array([0.02, 0.05, 0.08, 0.10, 0.12, 0.14])\n",
    "n_rays = len(p_values)\n",
    "\n",
    "G = build_G(p_values, v_true, dz)\n",
    "\n",
    "print(\"G matrix (path lengths in km):\")\n",
    "print(f\"{'p (s/km)':<12}\", ''.join(f'Layer {j+1:>5}' for j in range(n_layers)))\n",
    "for i in range(n_rays):\n",
    "    print(f\"{p_values[i]:<12.2f}\", ''.join(f'{G[i,j]:>9.2f}' for j in range(n_layers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hkpz204gr04",
   "metadata": {},
   "source": [
    "We set up a 4-layer velocity model and 6 rays with different ray parameters. The `build_G` function computes how far each ray travels in each layer — this fills the $\\mathbf{G}$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fd5b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the G matrix and ray paths side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Left: G matrix as image\n",
    "im = axes[0].imshow(G, aspect='auto', cmap='YlOrRd')\n",
    "axes[0].set_xlabel('Layer index')\n",
    "axes[0].set_ylabel('Ray index')\n",
    "axes[0].set_xticks(range(n_layers))\n",
    "axes[0].set_xticklabels([f'Layer {j+1}' for j in range(n_layers)])\n",
    "axes[0].set_yticks(range(n_rays))\n",
    "axes[0].set_yticklabels([f'p={p:.2f}' for p in p_values])\n",
    "axes[0].set_title('G matrix (path length, km)')\n",
    "for i in range(n_rays):\n",
    "    for j in range(n_layers):\n",
    "        if G[i, j] > 0:\n",
    "            axes[0].text(j, i, f'{G[i,j]:.1f}', ha='center', va='center', fontsize=8)\n",
    "plt.colorbar(im, ax=axes[0], shrink=0.8)\n",
    "\n",
    "# Right: ray paths\n",
    "ax = axes[1]\n",
    "colors = plt.cm.tab10(np.linspace(0, 0.6, n_rays))\n",
    "z_interfaces = np.concatenate([[0], np.cumsum(dz)])\n",
    "for z_int in z_interfaces:\n",
    "    ax.axhline(z_int, color='k', lw=0.5, ls='--', alpha=0.5)\n",
    "for j in range(n_layers):\n",
    "    z_mid = z_interfaces[j] + dz[j] / 2\n",
    "    ax.text(18, z_mid, f'v={v_true[j]} km/s', va='center', fontsize=9,\n",
    "            bbox=dict(boxstyle='round', fc='wheat', alpha=0.8))\n",
    "\n",
    "for i, p in enumerate(p_values):\n",
    "    x, z = 0.0, 0.0\n",
    "    x_pts, z_pts = [0.0], [0.0]\n",
    "    for j in range(n_layers):\n",
    "        if p >= 1.0 / v_true[j]:\n",
    "            break\n",
    "        cos_theta = np.sqrt(1 - (p * v_true[j])**2)\n",
    "        dx = dz[j] * p * v_true[j] / cos_theta\n",
    "        x += dx\n",
    "        z += dz[j]\n",
    "        x_pts.append(x)\n",
    "        z_pts.append(z)\n",
    "    x_up = [2 * x_pts[-1] - xi for xi in reversed(x_pts)]\n",
    "    z_up = list(reversed(z_pts))\n",
    "    ax.plot(x_pts + x_up[1:], z_pts + z_up[1:], '-', color=colors[i], lw=2, label=f'p={p:.2f}')\n",
    "\n",
    "ax.set_ylim(sum(dz) + 1, -1)\n",
    "ax.set_xlim(-0.5, 22)\n",
    "ax.set_xlabel('Distance (km)')\n",
    "ax.set_ylabel('Depth (km)')\n",
    "ax.set_title('Ray paths through the model')\n",
    "ax.legend(fontsize=8, loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k5jewzcw299",
   "metadata": {},
   "source": [
    "The left panel shows the $\\mathbf{G}$ matrix as an image — each entry is a path length (km). The right panel shows the ray paths through the layers. Notice that steeper rays (small $p$) penetrate deeper, while shallow rays (large $p$) turn before reaching the bottom layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a4ddd4",
   "metadata": {},
   "source": [
    "### Solving the inverse problem\n",
    "\n",
    "With more rays than layers ($N > M$), the system is **overdetermined**. The least-squares solution minimizes $||\\mathbf{Gm} - \\mathbf{d}||^2$:\n",
    "\n",
    "$$\\mathbf{m} = (\\mathbf{G}^T \\mathbf{G})^{-1} \\mathbf{G}^T \\mathbf{d}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa3627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Forward problem — compute synthetic data (no noise)\n",
    "d = G @ s_true\n",
    "\n",
    "# ===== KEY STEP: Inversion =====\n",
    "# m = (G^T G)^{-1} G^T d\n",
    "s_recovered = np.linalg.inv(G.T @ G) @ G.T @ d\n",
    "# ================================\n",
    "\n",
    "v_recovered = 1.0 / s_recovered\n",
    "\n",
    "print(f\"{'Layer':<8} {'v_true (km/s)':<16} {'v_recovered (km/s)':<20} {'Error (%)':<10}\")\n",
    "for j in range(n_layers):\n",
    "    err = 100 * abs(v_recovered[j] - v_true[j]) / v_true[j]\n",
    "    print(f\"{j+1:<8} {v_true[j]:<16.2f} {v_recovered[j]:<20.4f} {err:<10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ad2bb3",
   "metadata": {},
   "source": [
    "With perfect data, we recover the model exactly. But real data always has noise.\n",
    "\n",
    "### Effect of noise and smoothing\n",
    "\n",
    "With noisy data, the least-squares solution can become unstable. **Regularized least squares** adds a penalty that prefers **smooth models** — neighboring layers should have similar velocities unless the data strongly require a jump:\n",
    "\n",
    "$$\\mathbf{m} = (\\mathbf{G}^T \\mathbf{G} + \\alpha^2 \\mathbf{L}^T \\mathbf{L})^{-1} \\mathbf{G}^T \\mathbf{d}$$\n",
    "\n",
    "where $\\mathbf{L}$ is a **smoothing matrix** (finite-difference operator between adjacent layers) and $\\alpha$ controls the strength:\n",
    "\n",
    "- $\\alpha \\to 0$: pure least-squares (fits data closely, but noisy model)\n",
    "- $\\alpha$ large: heavily smoothed (smooth model, but poor data fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280c2185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to the data\n",
    "np.random.seed(3)\n",
    "noise_level = 0.05  # seconds\n",
    "d_noisy = d + noise_level * np.random.randn(n_rays)\n",
    "\n",
    "# Build 1D smoothing matrix\n",
    "L1d = smoothing_matrix_1d(n_layers)\n",
    "LtL_1d = L1d.T @ L1d\n",
    "\n",
    "# ===== KEY STEP: Inversion with different smoothing strengths =====\n",
    "# Undamped: m = (G^T G)^{-1} G^T d\n",
    "s_undamped = np.linalg.inv(G.T @ G) @ G.T @ d_noisy\n",
    "\n",
    "# Smoothed: m = (G^T G + alpha^2 L^T L)^{-1} G^T d\n",
    "alphas = [0.05, 0.5, 50.0]\n",
    "s_smooth = {}\n",
    "for alpha in alphas:\n",
    "    s_smooth[alpha] = np.linalg.inv(G.T @ G + alpha**2 * LtL_1d) @ G.T @ d_noisy\n",
    "# ==================================================================\n",
    "\n",
    "# ----- Plotting (you can ignore the details below) -----\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 5), sharey=True)\n",
    "\n",
    "cases = [('Undamped', s_undamped)] + [(f'$\\\\alpha$ = {a}', s_smooth[a]) for a in alphas]\n",
    "for ax, (label, s_inv) in zip(axes, cases):\n",
    "    v_inv = 1.0 / np.clip(s_inv, 1e-6, None)\n",
    "    ax.step(np.concatenate([[v_true[0]], v_true]), z_interfaces, 'k-', lw=2, label='True', where='pre')\n",
    "    ax.step(np.concatenate([[v_inv[0]], v_inv]), z_interfaces, 'r--', lw=2, label='Recovered', where='pre')\n",
    "    ax.set_xlabel('Velocity (km/s)')\n",
    "    ax.set_title(label)\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.set_xlim(0, 12)\n",
    "\n",
    "axes[0].set_ylabel('Depth (km)')\n",
    "axes[0].invert_yaxis()\n",
    "fig.suptitle(f'Effect of smoothing (noise = {noise_level} s)', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0585bfaa",
   "metadata": {},
   "source": [
    "### Try it yourself\n",
    "\n",
    "Change `noise_level` and `alphas` in the cell above. What happens when noise is very large? What smoothing strength gives the best recovery?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd85cf9",
   "metadata": {},
   "source": [
    "## 2. 2D Block Tomography\n",
    "\n",
    "The 1D inversion assumed the Earth varies only with depth. In reality, velocity varies **laterally** too. We divide the Earth into a 2D grid of $N_x \\times N_z$ blocks, each with unknown slowness $s_k$:\n",
    "\n",
    "$$t_i = \\sum_{k=1}^{M} G_{ik} \\, s_k$$\n",
    "\n",
    "where $G_{ik}$ is the **length of ray $i$ inside block $k$**. For simplicity, we trace **straight rays** between sources and receivers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9080d05f",
   "metadata": {},
   "source": [
    "### Textbook example: 20x20 grid (Shearer Exercise 2)\n",
    "\n",
    "The textbook provides a pre-computed $\\mathbf{G}$ matrix and travel time data for a 20x20 block model (400 unknowns) with 118 rays:\n",
    "- **20 horizontal** rays (one per row)\n",
    "- **20 vertical** rays (one per column)\n",
    "- **39 diagonal** rays at +45 degrees\n",
    "- **39 diagonal** rays at -45 degrees\n",
    "\n",
    "Having four ray directions gives good **angular coverage**, which is critical for resolving 2D structure. This is the same setup you will use in the homework.\n",
    "\n",
    "We invert for slowness perturbations using smoothing regularization:\n",
    "\n",
    "$$\\delta \\mathbf{s} = (\\mathbf{G}^T \\mathbf{G} + \\alpha^2 \\mathbf{L}^T \\mathbf{L})^{-1} \\mathbf{G}^T \\, \\delta \\mathbf{d}$$\n",
    "\n",
    "where $\\mathbf{L}$ penalizes differences between adjacent blocks (prefers smooth models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1na5i5mq5rd",
   "metadata": {},
   "source": [
    "The textbook provides a pre-computed $\\mathbf{G}$ matrix stored in a sparse format (only non-zero entries). The cell below loads it into a full matrix. **Just run it** — the file I/O details aren't important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df750638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load textbook G matrix (sparse format: ray_index, block_index, path_length)\n",
    "nx2, nz2 = 20, 20\n",
    "n_blocks2 = nx2 * nz2  # 400 unknowns\n",
    "\n",
    "# Each row of the file is: [ray_number, block_number, path_length_in_that_block]\n",
    "gmat_raw = np.loadtxt('tomography_data/tomo_gmat.txt')\n",
    "n_rays2 = int(gmat_raw[:, 0].max())  # 118 rays total\n",
    "\n",
    "# Build full G matrix from sparse entries\n",
    "G2 = np.zeros((n_rays2, n_blocks2))\n",
    "for ray_i, mod_j, path_len in gmat_raw:\n",
    "    G2[int(ray_i) - 1, int(mod_j) - 1] = path_len  # -1 for 0-based indexing\n",
    "\n",
    "print(f\"Grid: {nx2}x{nz2} = {n_blocks2} blocks\")\n",
    "print(f\"Rays: {n_rays2} (20 horizontal + 20 vertical + 39+39 diagonal)\")\n",
    "print(f\"G matrix: {G2.shape}, non-zeros: {np.count_nonzero(G2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dddf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the four ray types\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "ray_groups = [\n",
    "    (range(0, 20), 'Horizontal (20)'),\n",
    "    (range(20, 40), 'Vertical (20)'),\n",
    "    (range(40, 79), 'Diagonal +45\\u00b0 (39)'),\n",
    "    (range(79, 118), 'Diagonal \\u221245\\u00b0 (39)'),\n",
    "]\n",
    "\n",
    "for ax, (rays, title) in zip(axes, ray_groups):\n",
    "    for i in range(nx2 + 1):\n",
    "        ax.axvline(i, color='gray', lw=0.3)\n",
    "    for j in range(nz2 + 1):\n",
    "        ax.axhline(j, color='gray', lw=0.3)\n",
    "\n",
    "    for r in rays:\n",
    "        blocks = np.where(G2[r] > 0)[0]\n",
    "        if len(blocks) == 0:\n",
    "            continue\n",
    "        rows, cols = blocks // nx2, blocks % nx2\n",
    "        if title.startswith('Horizontal'):\n",
    "            order = np.argsort(cols)\n",
    "            ax.plot([cols[order[0]] + 0.5, cols[order[-1]] + 0.5],\n",
    "                    [rows[0] + 0.5, rows[0] + 0.5], 'b-', alpha=0.4, lw=0.8)\n",
    "        elif title.startswith('Vertical'):\n",
    "            order = np.argsort(rows)\n",
    "            ax.plot([cols[0] + 0.5, cols[0] + 0.5],\n",
    "                    [rows[order[0]] + 0.5, rows[order[-1]] + 0.5], 'r-', alpha=0.4, lw=0.8)\n",
    "        else:\n",
    "            order = np.argsort(cols)\n",
    "            ax.plot([cols[order[0]] + 0.5, cols[order[-1]] + 0.5],\n",
    "                    [rows[order[0]] + 0.5, rows[order[-1]] + 0.5], 'g-', alpha=0.4, lw=0.8)\n",
    "\n",
    "    ax.set_xlim(0, nx2)\n",
    "    ax.set_ylim(nz2, 0)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(title, fontsize=10)\n",
    "    ax.set_xlabel('Column')\n",
    "\n",
    "axes[0].set_ylabel('Row')\n",
    "plt.suptitle('Textbook ray geometry (Shearer Fig. 5.8)', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2144cvd2kxt",
   "metadata": {},
   "source": [
    "The plot below shows the four types of rays. Notice that horizontal and vertical rays each cover one row/column of blocks, while diagonal rays cut across at 45°. Having rays at **multiple angles** is key to resolving 2D structure — a single direction can only see averages along that direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6740e235",
   "metadata": {},
   "source": [
    "### Spike test (point-spread function)\n",
    "\n",
    "Place a single anomalous block in the model and see how the inversion smears it. This reveals the **point-spread function** — if we can't recover a single point perfectly, we can't resolve fine details either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "svt915zyzpt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build 2D smoothing matrix for the 20x20 grid\n",
    "L2d_20 = smoothing_matrix_2d(nx2, nz2)\n",
    "LtL_20 = L2d_20.T @ L2d_20\n",
    "\n",
    "# Spike test: single anomalous block at center\n",
    "alpha = 0.0\n",
    "noise_level = 0.0\n",
    "\n",
    "m_spike = np.zeros(n_blocks2)\n",
    "m_spike[10 * nx2 + 10] = 1.0  # single anomaly at row 10, col 10\n",
    "\n",
    "np.random.seed(42)\n",
    "d_spike = G2 @ m_spike + noise_level * np.random.randn(n_rays2)\n",
    "\n",
    "# ===== KEY STEP: Inversion =====\n",
    "# m = (G^T G + alpha^2 L^T L)^{-1} G^T d\n",
    "m_spike_rec = np.linalg.inv(G2.T @ G2 + alpha**2 * LtL_20) @ G2.T @ d_spike\n",
    "# ================================\n",
    "\n",
    "# ----- Plotting (you can ignore the details below) -----\n",
    "fig, axes = plt.subplots(1, 2, figsize=(11, 5), constrained_layout=True)\n",
    "\n",
    "im0 = axes[0].imshow(m_spike.reshape(nz2, nx2), cmap='RdBu', aspect='equal',\n",
    "                      extent=[0, nx2, nz2, 0], vmin=-1, vmax=1)\n",
    "axes[0].set_title('True spike model')\n",
    "plt.colorbar(im0, ax=axes[0], shrink=0.8, label='Slowness perturbation')\n",
    "\n",
    "im1 = axes[1].imshow(m_spike_rec.reshape(nz2, nx2), cmap='RdBu', aspect='equal',\n",
    "                      extent=[0, nx2, nz2, 0], vmin=-0.3, vmax=0.3)\n",
    "axes[1].set_title(f'Recovered ($\\\\alpha$={alpha}, noise={noise_level})')\n",
    "plt.colorbar(im1, ax=axes[1], shrink=0.8, label='Slowness perturbation')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('Column')\n",
    "    ax.set_ylabel('Row')\n",
    "plt.suptitle('Spike test: how a point anomaly gets smeared', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z6c06x3ubm",
   "metadata": {},
   "source": [
    "### Try it yourself\n",
    "\n",
    "Change `alpha` and `noise_level` in the cell above. What happens when `alpha = 0`? Why does it fail? Try moving the spike to a corner block (e.g., `m_spike[0] = 1.0`) — how does the recovery change for blocks at the edge vs. the center?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83nn2u8b9n",
   "metadata": {},
   "source": [
    "### Checkerboard test\n",
    "\n",
    "A more realistic resolution test: alternate fast and slow blocks in a checkerboard pattern, generate synthetic data (with noise), and see how well the inversion recovers the pattern. Regions that are well-resolved will show a clear checkerboard; poorly-resolved regions will be smeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ztmk2mtrot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkerboard test\n",
    "alpha = 0.001\n",
    "noise_level = 5.0\n",
    "\n",
    "# Build checkerboard: alternating +1/-1 in 4x4 blocks\n",
    "m_checker2 = np.zeros((nz2, nx2))\n",
    "block_size = 4\n",
    "for iz in range(nz2):\n",
    "    for ix in range(nx2):\n",
    "        if ((iz // block_size) + (ix // block_size)) % 2 == 0:\n",
    "            m_checker2[iz, ix] = 1.0\n",
    "        else:\n",
    "            m_checker2[iz, ix] = -1.0\n",
    "\n",
    "np.random.seed(42)\n",
    "d_checker2 = G2 @ m_checker2.ravel() + noise_level * np.random.randn(n_rays2)\n",
    "\n",
    "# ===== KEY STEP: Inversion =====\n",
    "# m = (G^T G + alpha^2 L^T L)^{-1} G^T d\n",
    "m_checker_rec = np.linalg.inv(G2.T @ G2 + alpha**2 * LtL_20) @ G2.T @ d_checker2\n",
    "# ================================\n",
    "\n",
    "# ----- Plotting (you can ignore the details below) -----\n",
    "fig, axes = plt.subplots(1, 2, figsize=(11, 5), constrained_layout=True)\n",
    "\n",
    "im0 = axes[0].imshow(m_checker2, cmap='RdBu', aspect='equal',\n",
    "                      extent=[0, nx2, nz2, 0], vmin=-1, vmax=1)\n",
    "axes[0].set_title('True checkerboard')\n",
    "plt.colorbar(im0, ax=axes[0], shrink=0.8, label='Slowness perturbation')\n",
    "\n",
    "im1 = axes[1].imshow(m_checker_rec.reshape(nz2, nx2), cmap='RdBu', aspect='equal',\n",
    "                      extent=[0, nx2, nz2, 0], vmin=-1, vmax=1)\n",
    "axes[1].set_title(f'Recovered ($\\\\alpha$={alpha}, noise={noise_level})')\n",
    "plt.colorbar(im1, ax=axes[1], shrink=0.8, label='Slowness perturbation')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('Column')\n",
    "    ax.set_ylabel('Row')\n",
    "plt.suptitle('Checkerboard test: can the inversion resolve the pattern?', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0puu1a2sts7",
   "metadata": {},
   "source": [
    "### Effect of smoothing strength\n",
    "\n",
    "The smoothing parameter $\\alpha$ controls the balance between fitting the data and keeping the model smooth. Too little smoothing amplifies noise; too much smoothing smears out the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0piq0joem569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of smoothing on checkerboard recovery\n",
    "alpha_values = [0.005, 5.0, 50.0]\n",
    "\n",
    "# ----- Plotting (you can ignore the details below) -----\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4), constrained_layout=True)\n",
    "\n",
    "# True model\n",
    "im0 = axes[0].imshow(m_checker2, cmap='RdBu', aspect='equal',\n",
    "                      extent=[0, nx2, nz2, 0], vmin=-1, vmax=1)\n",
    "axes[0].set_title('True checkerboard')\n",
    "axes[0].set_ylabel('Row')\n",
    "\n",
    "for ax, alpha in zip(axes[1:], alpha_values):\n",
    "    # ===== KEY STEP: Inversion with different alpha =====\n",
    "    m_rec = np.linalg.inv(G2.T @ G2 + alpha**2 * LtL_20) @ G2.T @ d_checker2\n",
    "    # ====================================================\n",
    "    im = ax.imshow(m_rec.reshape(nz2, nx2), cmap='RdBu', aspect='equal',\n",
    "                   extent=[0, nx2, nz2, 0], vmin=-1, vmax=1)\n",
    "    ax.set_title(f'$\\\\alpha$ = {alpha}')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('Column')\n",
    "plt.colorbar(im, ax=axes, label='Slowness perturbation', shrink=0.8)\n",
    "fig.suptitle('Effect of smoothing on checkerboard recovery (with noise)', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1i9epz6px0u",
   "metadata": {},
   "source": [
    "### Try it yourself\n",
    "\n",
    "Change `noise_level`, `alpha`, and `block_size` in the checkerboard cell above, and `alpha_values` in the smoothing comparison cell. What happens with smaller blocks (e.g., `block_size=2`)? Can the inversion still resolve the pattern?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834371f6",
   "metadata": {},
   "source": [
    "### Inverting real data\n",
    "\n",
    "In the synthetic tests above, we **computed** the data: $\\mathbf{d} = \\mathbf{G} \\mathbf{m}_{\\text{true}}$ (+ noise). Now we **load** real travel time observations — the same $\\mathbf{G}$ matrix, but with measured $\\delta \\mathbf{d}$ from the textbook. Unlike synthetic tests, we don't know the true model.\n",
    "\n",
    "Invert the data using the same formula and try different $\\alpha$ values:\n",
    "\n",
    "$$\\delta \\mathbf{s} = (\\mathbf{G}^T \\mathbf{G} + \\alpha^2 \\mathbf{L}^T \\mathbf{L})^{-1} \\mathbf{G}^T \\, \\delta \\mathbf{d}$$\n",
    "\n",
    "Think about:\n",
    "- Which features appear consistently across different $\\alpha$ values? Those are likely real.\n",
    "- Which features change dramatically? Those may be artifacts.\n",
    "- What happens when $\\alpha = 0$? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6uv7orzcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real travel time data\n",
    "data_raw = np.loadtxt('tomography_data/tomo_data.txt')\n",
    "d2 = data_raw[:, 1]\n",
    "print(f\"Data: {len(d2)} travel time perturbations, {np.sum(d2 != 0)} non-zero\")\n",
    "\n",
    "# YOUR CODE HERE: invert d2 using np.linalg.inv(G2.T @ G2 + alpha**2 * LtL_20) @ G2.T @ d2\n",
    "# Try several alpha values and plot the results with imshow\n",
    "alpha = 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654ffba9",
   "metadata": {},
   "source": [
    "## 3. Global Tomography Models\n",
    "\n",
    "The toy inversions used synthetic data on small grids. Real seismic tomography applies the same principles to millions of travel times. The result: 3D images of Earth's interior.\n",
    "\n",
    "### S362ANI: Global shear-wave velocity\n",
    "\n",
    "**S362ANI** (Kustowski et al., 2008) is a global model of shear-wave velocity perturbations from 25 km to the core-mantle boundary (2890 km), available from the [IRIS Earth Model Collaboration](https://ds.iris.edu/ds/products/emc-earthmodels/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c5a2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import netCDF4 as nc\n",
    "\n",
    "ds = nc.Dataset('tomography_data/S362ANI_percent.nc')\n",
    "lat = ds.variables['latitude'][:]\n",
    "lon = ds.variables['longitude'][:]\n",
    "depth = ds.variables['depth'][:]\n",
    "dvs = np.ma.filled(ds.variables['dvs'][:], fill_value=np.nan)\n",
    "\n",
    "print(f\"Model grid: {len(lat)} lat x {len(lon)} lon x {len(depth)} depths\")\n",
    "print(f\"Depth levels (km): {depth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4467c981",
   "metadata": {},
   "source": [
    "Each map shows $\\delta V_s / V_s$ (%) at a fixed depth. **Blue = fast** (cold, dense material), **Red = slow** (hot, buoyant material)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cy2ol5fu6",
   "metadata": {},
   "source": [
    "The plotting code below uses cartopy for map projections. **Just run it** and focus on interpreting the maps — the plotting details aren't important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab20cd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load plate boundaries (Bird, 2003)\n",
    "with open('tomography_data/PB2002_boundaries.json') as f:\n",
    "    pb_data = json.load(f)\n",
    "\n",
    "def plot_plate_boundaries(ax, color='k', lw=0.8, alpha=0.6):\n",
    "    \"\"\"Overlay PB2002 plate boundaries on a cartopy axis.\"\"\"\n",
    "    for feature in pb_data['features']:\n",
    "        coords = feature['geometry']['coordinates']\n",
    "        if feature['geometry']['type'] == 'LineString':\n",
    "            coords = [coords]\n",
    "        for segment in coords:\n",
    "            lons, lats = zip(*segment)\n",
    "            ax.plot(lons, lats, '-', color=color, lw=lw, alpha=alpha,\n",
    "                    transform=ccrs.PlateCarree())\n",
    "\n",
    "depth_slices = [100, 250, 600, 2800]  # km\n",
    "LON, LAT = np.meshgrid(lon, lat)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10),\n",
    "                         subplot_kw={'projection': ccrs.Robinson()})\n",
    "\n",
    "for ax, target_depth in zip(axes.ravel(), depth_slices):\n",
    "    iz = np.argmin(np.abs(depth - target_depth))\n",
    "    data = dvs[iz]\n",
    "    vmax = np.nanpercentile(np.abs(data), 98)\n",
    "\n",
    "    im = ax.pcolormesh(LON, LAT, data, cmap='RdBu', vmin=-vmax, vmax=vmax,\n",
    "                       transform=ccrs.PlateCarree(), shading='auto')\n",
    "    ax.coastlines(lw=0.5)\n",
    "    plot_plate_boundaries(ax)\n",
    "    ax.set_title(f'Depth = {depth[iz]:.0f} km', fontsize=12)\n",
    "    plt.colorbar(im, ax=ax, orientation='horizontal', pad=0.05,\n",
    "                 label='$\\\\delta V_s / V_s$ (%)', shrink=0.7)\n",
    "\n",
    "fig.suptitle('S362ANI: Global shear-wave velocity perturbations', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88193f2",
   "metadata": {},
   "source": [
    "**What to notice:**\n",
    "- **100 km**: Continents are fast (thick, cold lithosphere); ocean ridges are slow (hot upwelling)\n",
    "- **250 km**: Slow anomalies under East Africa (hotspot), fast under cratonic continents\n",
    "- **600 km**: Subducting slabs appear as fast anomalies (e.g., western Pacific)\n",
    "- **2800 km**: Near the core-mantle boundary — large low-velocity provinces under Africa and the Pacific (\"LLSVPs\")\n",
    "\n",
    "### Cross-sections through the Japan subduction zone\n",
    "\n",
    "A vertical slice reveals how a subducting slab descends into the mantle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l6am0w7rwf9",
   "metadata": {},
   "source": [
    "The code below plots vertical cross-sections through the Japan subduction zone. **Just run it** — the cartopy/matplotlib details are complex but just produce the map and slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b977799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-sections through the Japan subduction zone at three latitudes\n",
    "lat_slices = [44, 36, 28]  # Hokkaido, central Honshu, Ryukyu (north to south)\n",
    "lon_range = (125, 155)\n",
    "lon_mask = (lon >= lon_range[0]) & (lon <= lon_range[1])\n",
    "lon_sub = lon[lon_mask]\n",
    "depth_mask = depth <= 700\n",
    "depth_sub = depth[depth_mask]\n",
    "\n",
    "colors = ['C0', 'C1', 'C2']\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Map panel (left)\n",
    "ax_map = fig.add_subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "ax_map.set_extent([122, 158, 22, 50], crs=ccrs.PlateCarree())\n",
    "ax_map.coastlines(lw=0.8)\n",
    "ax_map.add_feature(cfeature.LAND, facecolor='lightgray', alpha=0.5)\n",
    "ax_map.add_feature(cfeature.OCEAN, facecolor='lightskyblue', alpha=0.2)\n",
    "for lat_s, c in zip(lat_slices, colors):\n",
    "    ax_map.plot([lon_range[0], lon_range[1]], [lat_s, lat_s],\n",
    "                '-', color=c, lw=2.5, transform=ccrs.PlateCarree(), zorder=5)\n",
    "    ax_map.text(lon_range[1]+0.5, lat_s, f'{lat_s}\\u00b0N', fontsize=10, va='center',\n",
    "                transform=ccrs.PlateCarree(), color=c, fontweight='bold')\n",
    "gl = ax_map.gridlines(draw_labels=True, lw=0.5, alpha=0.5)\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "ax_map.set_title('Cross-section locations')\n",
    "\n",
    "# Cross-section panels (right, stacked north to south)\n",
    "LON_xs, DEPTH_xs = np.meshgrid(lon_sub, depth_sub)\n",
    "gs = fig.add_gridspec(len(lat_slices), 2, width_ratios=[1, 1.5], wspace=0.35, hspace=0.35)\n",
    "\n",
    "for i, (lat_s, c) in enumerate(zip(lat_slices, colors)):\n",
    "    ilat = np.argmin(np.abs(lat - lat_s))\n",
    "    dvs_slice = dvs[:, ilat, lon_mask][depth_mask, :]\n",
    "    vmax = np.nanpercentile(np.abs(dvs_slice), 95)\n",
    "\n",
    "    ax = fig.add_subplot(gs[i, 1])\n",
    "    im = ax.pcolormesh(LON_xs, DEPTH_xs, dvs_slice, cmap='RdBu',\n",
    "                       vmin=-vmax, vmax=vmax, shading='auto')\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_ylabel('Depth (km)')\n",
    "    ax.set_title(f'{lat_s}\\u00b0N', color=c, fontweight='bold', fontsize=11)\n",
    "    plt.colorbar(im, ax=ax, label='$\\\\delta V_s / V_s$ (%)', shrink=0.9)\n",
    "    if i == len(lat_slices) - 1:\n",
    "        ax.set_xlabel('Longitude (\\u00b0E)')\n",
    "    else:\n",
    "        ax.set_xticklabels([])\n",
    "\n",
    "plt.suptitle('S362ANI: Cross-sections through the Japan subduction zone', fontsize=13, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3154c3",
   "metadata": {},
   "source": [
    "### MITPS-20: Regional P-wave velocity beneath North America\n",
    "\n",
    "**MITPS-20** (Golos et al., 2020) is a higher-resolution model of P-wave velocity at depths from 20 to 300 km."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fj3owmybqac",
   "metadata": {},
   "source": [
    "The code below plots depth slices of P-wave velocity beneath North America. **Just run it** and focus on interpreting the velocity patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad50c4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = nc.Dataset('tomography_data/MITPS-20.nc')\n",
    "lat2 = ds2.variables['latitude'][:]\n",
    "lon2 = ds2.variables['longitude'][:]\n",
    "depth2 = ds2.variables['depth'][:]\n",
    "dvp = np.ma.filled(ds2.variables['dVp'][:], fill_value=np.nan)\n",
    "\n",
    "depth_slices = [60, 100, 160, 200]\n",
    "LON2, LAT2 = np.meshgrid(lon2, lat2)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10),\n",
    "                         subplot_kw={'projection': ccrs.AlbersEqualArea(\n",
    "                             central_longitude=-95, central_latitude=40)})\n",
    "\n",
    "for ax, target_depth in zip(axes.ravel(), depth_slices):\n",
    "    iz = np.argmin(np.abs(depth2 - target_depth))\n",
    "    data = dvp[iz]\n",
    "    vmax = np.nanpercentile(np.abs(data), 95)\n",
    "\n",
    "    im = ax.pcolormesh(LON2, LAT2, data, cmap='RdBu', vmin=-vmax, vmax=vmax,\n",
    "                       transform=ccrs.PlateCarree(), shading='auto')\n",
    "    ax.coastlines(lw=0.5)\n",
    "    ax.add_feature(cfeature.BORDERS, lw=0.3)\n",
    "    ax.add_feature(cfeature.STATES, lw=0.2)\n",
    "    ax.plot(-122.27, 37.87, 'k^', ms=8, transform=ccrs.PlateCarree())\n",
    "    ax.set_title(f'Depth = {depth2[iz]:.0f} km', fontsize=12)\n",
    "    plt.colorbar(im, ax=ax, orientation='horizontal', pad=0.05,\n",
    "                 label='$\\\\delta V_p / V_p$ (%)', shrink=0.7)\n",
    "\n",
    "fig.suptitle('MITPS-20: P-wave velocity perturbations beneath North America\\n(\\u25b2 = Berkeley)',\n",
    "             fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523bdf21",
   "metadata": {},
   "source": [
    "**What to notice:**\n",
    "- The western US is broadly **slow** (hot, actively deforming: Basin & Range, Yellowstone)\n",
    "- The eastern US / Canadian Shield is **fast** (cold, thick, ancient cratonic lithosphere)\n",
    "- The Juan de Fuca slab appears as a fast anomaly under the Pacific Northwest (Cascadia subduction)\n",
    "- The Yellowstone hotspot shows as a prominent slow anomaly\n",
    "\n",
    "### Physical interpretation\n",
    "\n",
    "Seismic velocity perturbations are primarily controlled by **temperature**:\n",
    "\n",
    "| Anomaly | Velocity | Temperature | Physical interpretation |\n",
    "|---------|----------|-------------|------------------------|\n",
    "| Blue (fast) | $\\delta V > 0$ | Cold | Subducting slabs, cratonic lithosphere |\n",
    "| Red (slow) | $\\delta V < 0$ | Hot | Mantle plumes, mid-ocean ridges, active tectonics |\n",
    "\n",
    "Typical velocity perturbations are **1-5%**, corresponding to temperature variations of ~100-500 K.\n",
    "\n",
    "**Limitations of tomographic models:**\n",
    "- **Uneven ray coverage**: oceans have fewer stations, so lower resolution in oceanic mantle\n",
    "- **Regularization**: damping/smoothing means we can never recover the true Earth perfectly\n",
    "- **Trade-offs**: velocity vs. anisotropy, temperature vs. composition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7e3d41",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### The big picture\n",
    "\n",
    "Seismic tomography turns travel time data into images of Earth's interior. The same linear algebra — $\\mathbf{Gm} = \\mathbf{d}$ — works from a 4-layer toy model all the way up to 3D global imaging with millions of rays.\n",
    "\n",
    "What tomography has revealed:\n",
    "- **Subducting slabs**: cold oceanic plates sinking into the mantle (fast anomalies dipping from trenches)\n",
    "- **Mantle plumes / hotspots**: hot upwellings from the deep mantle (slow anomalies beneath Yellowstone, Hawaii, East Africa)\n",
    "- **LLSVPs**: two continent-sized slow structures at the base of the mantle beneath Africa and the Pacific\n",
    "- **Cratonic roots**: thick, cold, ancient lithosphere beneath stable continents (Canadian Shield, Siberia)\n",
    "\n",
    "### Key concepts\n",
    "\n",
    "| Concept | Key idea |\n",
    "|---------|----------|\n",
    "| Forward vs. inverse | Forward: given model, compute data. Inverse: given data, recover model |\n",
    "| $\\mathbf{Gm} = \\mathbf{d}$ | Path-length matrix $\\times$ slowness = travel times |\n",
    "| Smoothing regularization | $\\alpha^2 \\mathbf{L}^T \\mathbf{L}$ penalizes rough models — prefers smooth solutions |\n",
    "| Checkerboard test | Synthetic test showing which parts of the model can be resolved |\n",
    "| Spike test | Point-spread function — how a point anomaly gets smeared |\n",
    "| Ray coverage | Blocks sampled by many rays at different angles are best resolved |\n",
    "\n",
    "### Always check resolution before interpreting!\n",
    "\n",
    "A tomographic image is only as good as the data behind it. Before interpreting any feature:\n",
    "1. **Check ray coverage** — are there enough rays crossing that region at different angles?\n",
    "2. **Look at checkerboard/spike tests** — can the inversion resolve features of this size at this location?\n",
    "3. **Consider smoothing** — strong smoothing smears structure; weak smoothing creates artifacts\n",
    "\n",
    "If a feature doesn't pass these checks, it might be an artifact of poor coverage or regularization, not real Earth structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
